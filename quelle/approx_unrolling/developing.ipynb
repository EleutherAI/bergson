{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "from quelle.approx_unrolling.utils import TensorDict\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], dtype=torch.float16, requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(\n",
    "    1,\n",
    "    dtype=torch.float16,\n",
    "    requires_grad=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "\n",
    "model_str = \"EleutherAI/pythia-14m\"\n",
    "step = 5000\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    model_str,\n",
    "    revision=f\"step{step}\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_str,\n",
    "    revision=f\"step{step}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging gradient covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logger:logger.py:log:INFO:  Tracking modules with names: ['gpt_neox.layers.0.attention.query_key_value', 'gpt_neox.layers.0.attention.dense', 'gpt_neox.layers.0.mlp.dense_h_to_4h', 'gpt_neox.layers.0.mlp.dense_4h_to_h', 'gpt_neox.layers.1.attention.query_key_value', 'gpt_neox.layers.1.attention.dense', 'gpt_neox.layers.1.mlp.dense_h_to_4h', 'gpt_neox.layers.1.mlp.dense_4h_to_h', 'gpt_neox.layers.2.attention.query_key_value', 'gpt_neox.layers.2.attention.dense', 'gpt_neox.layers.2.mlp.dense_h_to_4h', 'gpt_neox.layers.2.mlp.dense_4h_to_h', 'gpt_neox.layers.3.attention.query_key_value', 'gpt_neox.layers.3.attention.dense', 'gpt_neox.layers.3.mlp.dense_h_to_4h', 'gpt_neox.layers.3.mlp.dense_4h_to_h', 'gpt_neox.layers.4.attention.query_key_value', 'gpt_neox.layers.4.attention.dense', 'gpt_neox.layers.4.mlp.dense_h_to_4h', 'gpt_neox.layers.4.mlp.dense_4h_to_h', 'gpt_neox.layers.5.attention.query_key_value', 'gpt_neox.layers.5.attention.dense', 'gpt_neox.layers.5.mlp.dense_h_to_4h', 'gpt_neox.layers.5.mlp.dense_4h_to_h'].\n",
      "logger:logger.py:log:INFO:  Initializing `Analyzer` with parameters: {'self': <quelle.hessians.analyzer.Analyzer object at 0x7ba9bd12fa90>, 'analysis_name': '', 'model': GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 128)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (query_key_value): TrackedLinear(\n",
      "            (original_module): Linear(in_features=128, out_features=384, bias=True)\n",
      "          )\n",
      "          (dense): TrackedLinear(\n",
      "            (original_module): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): TrackedLinear(\n",
      "            (original_module): Linear(in_features=128, out_features=512, bias=True)\n",
      "          )\n",
      "          (dense_4h_to_h): TrackedLinear(\n",
      "            (original_module): Linear(in_features=512, out_features=128, bias=True)\n",
      "          )\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  )\n",
      "  (embed_out): Linear(in_features=128, out_features=50304, bias=False)\n",
      "), 'task': <quelle.approx_unrolling.language_task.LanguageModelingTask object at 0x7bae1bfc9150>, 'cpu': False, 'log_level': None, 'log_main_process_only': True, 'profile': False, 'disable_tqdm': False, 'output_dir': './influence_results', 'disable_model_save': True, '__class__': <class 'quelle.hessians.analyzer.Analyzer'>}\n",
      "logger:logger.py:log:INFO:  Process state configuration:\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)  # For multi-GPU\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "from quelle.approx_unrolling.language_task import LanguageModelingTask\n",
    "from quelle.approx_unrolling.pile_data import get_pile_dataset\n",
    "from quelle.hessians.analyzer import prepare_model\n",
    "from quelle.hessians.arguments import FactorArguments\n",
    "\n",
    "\n",
    "module_keys = [m[0] for m in model.named_modules()]\n",
    "\n",
    "task = LanguageModelingTask(module_keys=module_keys)\n",
    "\n",
    "\n",
    "factors_name = \"ekfac\"\n",
    "factor_args = FactorArguments(strategy=factors_name)  # type:ignore\n",
    "factors_name += \"_half\"\n",
    "\n",
    "model = prepare_model(model, task)\n",
    "\n",
    "\n",
    "from quelle.hessians.analyzer import Analyzer\n",
    "from quelle.hessians.utils.dataset import DataLoaderKwargs\n",
    "from transformers import default_data_collator\n",
    "\n",
    "\n",
    "analyzer = Analyzer(\n",
    "    analysis_name=\"\",\n",
    "    model=model,\n",
    "    task=task,\n",
    ")\n",
    "\n",
    "module_partition_names, target_module_partitions = analyzer._get_module_partition(\n",
    "    module_partitions=factor_args.lambda_module_partitions,\n",
    "    target_module_partitions=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pile_data:pile_data.py:get_pile_dataset:INFO:  Loading Pile 10k dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pile_data:pile_data.py:get_pile_dataset:INFO:  Loading tokenizer for EleutherAI/pythia-14m at step 0...\n",
      "pile_data:pile_data.py:get_pile_dataset:INFO:  Limiting to 1 samples...\n",
      "pile_data:pile_data.py:get_pile_dataset:INFO:  Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrow_dataset:arrow_dataset.py:map:WARNING:  num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "pile_data:pile_data.py:get_pile_dataset:INFO:  Grouping texts into chunks of 2048 tokens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrow_dataset:arrow_dataset.py:map:WARNING:  num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "pile_data:pile_data.py:get_pile_dataset:INFO:  Final dataset size: 1 samples\n",
      "pile_data:pile_data.py:get_pile_dataset:INFO:  Each sample has 2048 tokens\n",
      "logger:logger.py:log:INFO:  Using the DataLoader parameters: {'num_workers': 0, 'collate_fn': <function default_data_collator at 0x7ba99811f9c0>, 'pin_memory': False, 'timeout': 0, 'worker_init_fn': None, 'multiprocessing_context': None, 'generator': None, 'prefetch_factor': None, 'persistent_workers': False, 'pin_memory_device': ''}.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = get_pile_dataset(model_str=\"EleutherAI/pythia-14m\", step=0, max_samples=1)\n",
    "dataloader_kwargs = DataLoaderKwargs(collate_fn=default_data_collator)\n",
    "analyzer.set_dataloader_kwargs(dataloader_kwargs)\n",
    "dataloader_params = analyzer._configure_dataloader(dataloader_kwargs)\n",
    "dataset = train_dataset\n",
    "\n",
    "if factor_args.covariance_max_examples is None:\n",
    "    total_data_examples = len(dataset)\n",
    "else:\n",
    "    total_data_examples = min([factor_args.covariance_max_examples, len(dataset)])\n",
    "\n",
    "data_partition_indices, target_data_partitions = analyzer._get_data_partition(\n",
    "    total_data_examples=total_data_examples,\n",
    "    data_partitions=factor_args.covariance_data_partitions,\n",
    "    target_data_partitions=None,\n",
    ")\n",
    "\n",
    "for data_partition in target_data_partitions:\n",
    "    start_index, end_index = data_partition_indices[data_partition]\n",
    "\n",
    "loader = analyzer._get_dataloader(\n",
    "    dataset=train_dataset,\n",
    "    per_device_batch_size=32,\n",
    "    dataloader_params=dataloader_params,\n",
    "    indices=list(range(start_index, end_index)),\n",
    "    allow_duplicates=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cov = analyzer.fit_covariance_matrices(\n",
    "#     factors_name=factors_name,\n",
    "#     dataset=train_dataset,\n",
    "#     per_device_batch_size=32,\n",
    "#     initial_per_device_batch_size_attempt=32,\n",
    "#     dataloader_kwargs=dataloader_kwargs,\n",
    "#     factor_args=factor_args,\n",
    "# )\n",
    "\n",
    "# cov_2 = analyzer.fit_covariance_matrices(\n",
    "#     factors_name=factors_name,\n",
    "#     dataset=train_dataset,\n",
    "#     per_device_batch_size=32,\n",
    "#     initial_per_device_batch_size_attempt=32,\n",
    "#     dataloader_kwargs=dataloader_kwargs,\n",
    "#     factor_args=factor_args,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting covariance matrices [0/1]   0%|           [time left: ?, time spent: 00:00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting covariance matrices [1/1] 100%|██████████ [time left: 00:00, time spent: 00:00]\n"
     ]
    }
   ],
   "source": [
    "from quelle.hessians.factor.covariance import fit_covariance_matrices_with_loader\n",
    "\n",
    "\n",
    "d = fit_covariance_matrices_with_loader(\n",
    "    model=analyzer.model,\n",
    "    state=analyzer.state,\n",
    "    task=analyzer.task,\n",
    "    loader=loader,\n",
    "    factor_args=factor_args,\n",
    "    tracked_module_names=module_partition_names[0],\n",
    "    disable_tqdm=analyzer.disable_tqdm,\n",
    ")[1]\n",
    "\n",
    "d[\"gradient_covariance\"].keys()\n",
    "\n",
    "test_key = \"gpt_neox.layers.0.attention.query_key_value\"\n",
    "\n",
    "activ_1 = TensorDict(d[\"activation_covariance\"])\n",
    "grad_1 = TensorDict(d[\"gradient_covariance\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting covariance matrices [1/1] 100%|██████████ [time left: 00:00, time spent: 00:00]\n"
     ]
    }
   ],
   "source": [
    "from quelle.hessians.factor.covariance import fit_covariance_matrices_with_loader\n",
    "\n",
    "\n",
    "d_2 = fit_covariance_matrices_with_loader(\n",
    "    model=analyzer.model,\n",
    "    state=analyzer.state,\n",
    "    task=analyzer.task,\n",
    "    loader=loader,\n",
    "    factor_args=factor_args,\n",
    "    tracked_module_names=module_partition_names[0],\n",
    "    disable_tqdm=analyzer.disable_tqdm,\n",
    ")[1]\n",
    "activ_2 = TensorDict(d_2[\"activation_covariance\"])\n",
    "grad_2 = TensorDict(d_2[\"gradient_covariance\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_activ = activ_1 - activ_2\n",
    "diff_grad = grad_1 - grad_2\n",
    "\n",
    "close_activ = activ_1.allclose(activ_2, atol=1e-4, rtol=1e-4)\n",
    "close_grad = grad_1.allclose(grad_2, atol=1e-4, rtol=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in close_activ.items():\n",
    "    if not v:\n",
    "        print(f\"activations differ for {k}: {diff_activ[k].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activations differ for gpt_neox.layers.0.attention.query_key_value: 11.477673530578613\n",
      "activations differ for gpt_neox.layers.0.attention.dense: 501.98358154296875\n",
      "activations differ for gpt_neox.layers.0.mlp.dense_h_to_4h: 20.246353149414062\n",
      "activations differ for gpt_neox.layers.0.mlp.dense_4h_to_h: 501.98358154296875\n",
      "activations differ for gpt_neox.layers.1.attention.query_key_value: 20.3133544921875\n",
      "activations differ for gpt_neox.layers.1.attention.dense: 351.5631103515625\n",
      "activations differ for gpt_neox.layers.1.mlp.dense_h_to_4h: 10.070480346679688\n",
      "activations differ for gpt_neox.layers.1.mlp.dense_4h_to_h: 351.5631103515625\n",
      "activations differ for gpt_neox.layers.2.attention.query_key_value: 44.117828369140625\n",
      "activations differ for gpt_neox.layers.2.attention.dense: 365.1512451171875\n",
      "activations differ for gpt_neox.layers.2.mlp.dense_h_to_4h: 28.146560668945312\n",
      "activations differ for gpt_neox.layers.2.mlp.dense_4h_to_h: 365.1512451171875\n",
      "activations differ for gpt_neox.layers.3.attention.query_key_value: 640.7262573242188\n",
      "activations differ for gpt_neox.layers.3.attention.dense: 115.5247802734375\n",
      "activations differ for gpt_neox.layers.3.mlp.dense_h_to_4h: 9.219436645507812\n",
      "activations differ for gpt_neox.layers.3.mlp.dense_4h_to_h: 115.5247802734375\n",
      "activations differ for gpt_neox.layers.4.attention.query_key_value: 18.026748657226562\n",
      "activations differ for gpt_neox.layers.4.attention.dense: 75.29647827148438\n",
      "activations differ for gpt_neox.layers.4.mlp.dense_h_to_4h: 5.45134162902832\n",
      "activations differ for gpt_neox.layers.4.mlp.dense_4h_to_h: 75.29647827148438\n",
      "activations differ for gpt_neox.layers.5.attention.query_key_value: 54.427398681640625\n",
      "activations differ for gpt_neox.layers.5.attention.dense: 55.704734802246094\n",
      "activations differ for gpt_neox.layers.5.mlp.dense_h_to_4h: 7.155542373657227\n",
      "activations differ for gpt_neox.layers.5.mlp.dense_4h_to_h: 55.704734802246094\n"
     ]
    }
   ],
   "source": [
    "for k, v in close_grad.items():\n",
    "    if not v:\n",
    "        print(f\"activations differ for {k}: {diff_grad[k].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = analyzer.state\n",
    "from accelerate.utils import find_batch_size, send_to_device\n",
    "from torch import GradScaler, autocast\n",
    "from quelle.hessians.module.utils import set_attention_mask\n",
    "from quelle.hessians.utils.state import State, no_sync\n",
    "\n",
    "enable_amp = True\n",
    "\n",
    "enable_grad_scaler = enable_amp and factor_args.amp_dtype == torch.float16\n",
    "\n",
    "scaler = GradScaler(device=\"cuda\", init_scale=factor_args.amp_scale, enabled=enable_grad_scaler)\n",
    "for index, batch in enumerate(loader):\n",
    "    batch = send_to_device(batch, device=state.device)\n",
    "\n",
    "    attention_mask = task.get_attention_mask(batch=batch)\n",
    "\n",
    "    if attention_mask is not None:\n",
    "        set_attention_mask(model=model, attention_mask=attention_mask)\n",
    "\n",
    "    with no_sync(model=model, state=state):\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        with autocast(\n",
    "            device_type=state.device.type,\n",
    "            enabled=enable_amp,\n",
    "            dtype=factor_args.amp_dtype,\n",
    "        ):\n",
    "            loss = task.compute_train_loss(\n",
    "                batch=batch,\n",
    "                model=model,\n",
    "                sample=True,\n",
    "            )\n",
    "\n",
    "        scaler.scale(loss).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9166.1602, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = list(model.modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation_eigenvectors': None,\n",
       " 'activation_eigenvalues': None,\n",
       " 'gradient_eigenvectors': None,\n",
       " 'gradient_eigenvalues': None,\n",
       " 'lambda_matrix': None,\n",
       " 'num_lambda_processed': None,\n",
       " 'activation_covariance': None,\n",
       " 'gradient_covariance': None,\n",
       " 'num_activation_covariance_processed': None,\n",
       " 'num_gradient_covariance_processed': None}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules[11].storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quelle.hessians.module.utils import load_factors\n",
    "from quelle.hessians.utils.constants import COVARIANCE_FACTOR_NAMES\n",
    "\n",
    "\n",
    "for factor_name in COVARIANCE_FACTOR_NAMES:\n",
    "    factor = load_factors(\n",
    "        model=model,\n",
    "        factor_name=factor_name,\n",
    "        tracked_module_names=None,\n",
    "        cpu=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=128, out_features=384, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=128, out_features=512, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=128, out_features=384, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=128, out_features=512, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=128, out_features=384, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=128, out_features=512, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=128, out_features=384, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=128, out_features=512, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=128, out_features=384, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=128, out_features=512, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=128, out_features=384, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=128, out_features=512, bias=True)\n",
      ")\n",
      "TrackedLinear(\n",
      "  (original_module): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from quelle.hessians.module.tracked_module import ModuleMode, TrackedModule\n",
    "\n",
    "for module in model.modules():\n",
    "    \n",
    "    if isinstance(module, TrackedModule):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exammining the matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_comparison(path_1, path_2):\n",
    "    files_1 = os.listdir(path_1)\n",
    "    files_2 = os.listdir(path_2)\n",
    "\n",
    "    for file_1 in files_1:\n",
    "        if file_1 in files_2:\n",
    "            if file_1.endswith(\".safetensors\"):\n",
    "                tensor_1 = TensorDict(\n",
    "                    load_file(\n",
    "                        os.path.join(path_1, file_1),\n",
    "                        device=\"cuda\",\n",
    "                    )\n",
    "                )\n",
    "                tensor_2 = TensorDict(\n",
    "                    load_file(\n",
    "                        os.path.join(path_2, file_1),\n",
    "                        device=\"cuda\",\n",
    "                    )\n",
    "                )\n",
    "                diff = tensor_1 - tensor_2\n",
    "                all_close = tensor_1.allclose(tensor_2, rtol=1e-5, atol=1e-5)\n",
    "                all_close_values = all(all_close.values())\n",
    "                if not all_close_values:\n",
    "                    print(file_1)\n",
    "                    print(\"Differences found:\")\n",
    "                    print(diff.max())\n",
    "                # check if all_close has any key that is False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_1 = \"/root/quelle/tests/caches/cache_1/.models/EleutherAI/pythia-14m/checkpoint_1000/influence_results/factors_ekfac_half\"\n",
    "path_2 = \"/root/quelle/tests/caches/cache_2/.models/EleutherAI/pythia-14m/checkpoint_1000/influence_results/factors_ekfac_half\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comparison(path_2, path_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comparison(path_2, path_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 = TensorDict(load_file(os.path.join(path_1, \"gradient_covariance.safetensors\"), device=\"cuda\"))\n",
    "tensor_2 = TensorDict(load_file(os.path.join(path_2, \"gradient_covariance.safetensors\"), device=\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tensor_1 - tensor_2).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in (tensor_1 - tensor_2).items():\n",
    "    print(k, v.max().item(), v.min().item(), v.mean().item(), v.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = TensorDict(\n",
    "    load_file(\n",
    "        \"/root/quelle/quelle/approx_unrolling/.models/EleutherAI/pythia-14m/segment_0/influence_results/factors_ekfac_half/average_gradient_covariance.safetensors\",\n",
    "        device=\"cuda\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "d_2 = TensorDict(\n",
    "    load_file(\n",
    "        \"/root/quelle/.models/EleutherAI/influence_results/factors_ekfac_half/gradient_covariance.safetensors\",\n",
    "        device=\"cuda\",\n",
    "    )\n",
    ")\n",
    "\n",
    "diff = d - d_2\n",
    "\n",
    "for k, v in diff.items():\n",
    "    if v.max() < 1e-5:\n",
    "        continue\n",
    "    print(k)\n",
    "    print(v.max())\n",
    "    print(\"----\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 15335424\n",
    "# determine prime decomposition of number\n",
    "number / 7488\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quelle.approx_unrolling.utils import TensorDict\n",
    "\n",
    "\n",
    "d = TensorDict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [d, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/louis/quelle/quelle/approx_unrolling/influence_results/wikitext/factors_ekfac_half\"\n",
    "\n",
    "# list of all subfolders or files\n",
    "import os\n",
    "\n",
    "subfolders = []\n",
    "for dirpath, dirnames, filenames in os.walk(path):\n",
    "    for dirname in dirnames:\n",
    "        subfolders.append(os.path.join(dirpath, dirname))\n",
    "    for filename in filenames:\n",
    "        subfolders.append(os.path.join(dirpath, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames\n",
    "filenames_json = [f for f in subfolders if f.endswith(\".json\")]\n",
    "filesnames_safetensors = [f for f in subfolders if f.endswith(\".safetensors\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_file = filenames_json[0]\n",
    "with open(json_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesnames_safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "for i in range(len(filesnames_safetensors)):\n",
    "    path = filesnames_safetensors[i]\n",
    "    tensors = {}\n",
    "    with safe_open(path, framework=\"pt\", device=0) as f:\n",
    "        for k in f.keys():\n",
    "            tensors[k] = f.get_tensor(k)\n",
    "    print(tensors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = filesnames_safetensors[-2]\n",
    "with safe_open(path, framework=\"pt\", device=0) as f:\n",
    "    for k in f.keys():\n",
    "        tensors[k] = f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_2 = torch.load(\"/home/louis/quelle/quelle/approx_unrolling/checkpoints/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = model.state_dict()\n",
    "\n",
    "all_mlps = {k: v for k, v in all_weights.items() if \"mlp\" in k}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_mlps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_keys = [m[0] for m in model.named_modules()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_attention = True\n",
    "track_mlp = True\n",
    "total_modules = []\n",
    "for m in module_keys:\n",
    "    if \"dropout\" in m.lower() or \"layernorm\" in m.lower():\n",
    "        continue\n",
    "\n",
    "    if \"attention\" in m.lower() and track_attention:\n",
    "        total_modules.append(m)\n",
    "    if \"mlp\" in m.lower() and track_mlp:\n",
    "        total_modules.append(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.wikitext.pipeline import get_wikitext_dataset\n",
    "\n",
    "train_dataset = get_wikitext_dataset(\n",
    "    split=\"eval_train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from train_dataset\n",
    "sample = train_dataset[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quelle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
