{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "\n",
    "model_str = \"EleutherAI/pythia-14m\"\n",
    "step = 143000\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    model_str,\n",
    "    revision=f\"step{step}\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_str,\n",
    "    revision=f\"step{step}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exammining the matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/louis/quelle/quelle/approx_unrolling/influence_results/wikitext/factors_ekfac_half\"\n",
    "\n",
    "# list of all subfolders or files\n",
    "import os\n",
    "\n",
    "subfolders = []\n",
    "for dirpath, dirnames, filenames in os.walk(path):\n",
    "    for dirname in dirnames:\n",
    "        subfolders.append(os.path.join(dirpath, dirname))\n",
    "    for filename in filenames:\n",
    "        subfolders.append(os.path.join(dirpath, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames\n",
    "filenames_json = [f for f in subfolders if f.endswith(\".json\")]\n",
    "filesnames_safetensors = [f for f in subfolders if f.endswith(\".safetensors\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/louis/quelle/quelle/approx_unrolling/influence_results/wikitext/factors_ekfac_half/factor_arguments.json',\n",
       " '/home/louis/quelle/quelle/approx_unrolling/influence_results/wikitext/factors_ekfac_half/covariance_dataset_metadata.json']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'strategy': 'ekfac',\n",
       " 'use_empirical_fisher': False,\n",
       " 'amp_dtype': 'torch.bfloat16',\n",
       " 'amp_scale': 65536.0,\n",
       " 'has_shared_parameters': False,\n",
       " 'covariance_max_examples': 100000,\n",
       " 'covariance_data_partitions': 1,\n",
       " 'covariance_module_partitions': 1,\n",
       " 'activation_covariance_dtype': 'torch.bfloat16',\n",
       " 'gradient_covariance_dtype': 'torch.bfloat16',\n",
       " 'eigendecomposition_dtype': 'torch.float64',\n",
       " 'lambda_max_examples': 100000,\n",
       " 'lambda_data_partitions': 1,\n",
       " 'lambda_module_partitions': 1,\n",
       " 'use_iterative_lambda_aggregation': False,\n",
       " 'offload_activations_to_cpu': False,\n",
       " 'per_sample_gradient_dtype': 'torch.bfloat16',\n",
       " 'lambda_dtype': 'torch.bfloat16'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_file = filenames_json[0]\n",
    "with open(json_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/louis/quelle/quelle/approx_unrolling/influence_results/wikitext/factors_ekfac_half/activation_covariance.safetensors',\n",
       " '/home/louis/quelle/quelle/approx_unrolling/influence_results/wikitext/factors_ekfac_half/gradient_covariance.safetensors',\n",
       " '/home/louis/quelle/quelle/approx_unrolling/influence_results/wikitext/factors_ekfac_half/num_activation_covariance_processed.safetensors',\n",
       " '/home/louis/quelle/quelle/approx_unrolling/influence_results/wikitext/factors_ekfac_half/num_gradient_covariance_processed.safetensors']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filesnames_safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['lm_head', 'transformer.h.0.attn.c_attn', 'transformer.h.0.attn.c_proj', 'transformer.h.0.mlp.c_fc', 'transformer.h.0.mlp.c_proj', 'transformer.h.1.attn.c_attn', 'transformer.h.1.attn.c_proj', 'transformer.h.1.mlp.c_fc', 'transformer.h.1.mlp.c_proj', 'transformer.h.10.attn.c_attn', 'transformer.h.10.attn.c_proj', 'transformer.h.10.mlp.c_fc', 'transformer.h.10.mlp.c_proj', 'transformer.h.11.attn.c_attn', 'transformer.h.11.attn.c_proj', 'transformer.h.11.mlp.c_fc', 'transformer.h.11.mlp.c_proj', 'transformer.h.2.attn.c_attn', 'transformer.h.2.attn.c_proj', 'transformer.h.2.mlp.c_fc', 'transformer.h.2.mlp.c_proj', 'transformer.h.3.attn.c_attn', 'transformer.h.3.attn.c_proj', 'transformer.h.3.mlp.c_fc', 'transformer.h.3.mlp.c_proj', 'transformer.h.4.attn.c_attn', 'transformer.h.4.attn.c_proj', 'transformer.h.4.mlp.c_fc', 'transformer.h.4.mlp.c_proj', 'transformer.h.5.attn.c_attn', 'transformer.h.5.attn.c_proj', 'transformer.h.5.mlp.c_fc', 'transformer.h.5.mlp.c_proj', 'transformer.h.6.attn.c_attn', 'transformer.h.6.attn.c_proj', 'transformer.h.6.mlp.c_fc', 'transformer.h.6.mlp.c_proj', 'transformer.h.7.attn.c_attn', 'transformer.h.7.attn.c_proj', 'transformer.h.7.mlp.c_fc', 'transformer.h.7.mlp.c_proj', 'transformer.h.8.attn.c_attn', 'transformer.h.8.attn.c_proj', 'transformer.h.8.mlp.c_fc', 'transformer.h.8.mlp.c_proj', 'transformer.h.9.attn.c_attn', 'transformer.h.9.attn.c_proj', 'transformer.h.9.mlp.c_fc', 'transformer.h.9.mlp.c_proj'])\n",
      "dict_keys(['lm_head', 'transformer.h.0.attn.c_attn', 'transformer.h.0.attn.c_proj', 'transformer.h.0.mlp.c_fc', 'transformer.h.0.mlp.c_proj', 'transformer.h.1.attn.c_attn', 'transformer.h.1.attn.c_proj', 'transformer.h.1.mlp.c_fc', 'transformer.h.1.mlp.c_proj', 'transformer.h.10.attn.c_attn', 'transformer.h.10.attn.c_proj', 'transformer.h.10.mlp.c_fc', 'transformer.h.10.mlp.c_proj', 'transformer.h.11.attn.c_attn', 'transformer.h.11.attn.c_proj', 'transformer.h.11.mlp.c_fc', 'transformer.h.11.mlp.c_proj', 'transformer.h.2.attn.c_attn', 'transformer.h.2.attn.c_proj', 'transformer.h.2.mlp.c_fc', 'transformer.h.2.mlp.c_proj', 'transformer.h.3.attn.c_attn', 'transformer.h.3.attn.c_proj', 'transformer.h.3.mlp.c_fc', 'transformer.h.3.mlp.c_proj', 'transformer.h.4.attn.c_attn', 'transformer.h.4.attn.c_proj', 'transformer.h.4.mlp.c_fc', 'transformer.h.4.mlp.c_proj', 'transformer.h.5.attn.c_attn', 'transformer.h.5.attn.c_proj', 'transformer.h.5.mlp.c_fc', 'transformer.h.5.mlp.c_proj', 'transformer.h.6.attn.c_attn', 'transformer.h.6.attn.c_proj', 'transformer.h.6.mlp.c_fc', 'transformer.h.6.mlp.c_proj', 'transformer.h.7.attn.c_attn', 'transformer.h.7.attn.c_proj', 'transformer.h.7.mlp.c_fc', 'transformer.h.7.mlp.c_proj', 'transformer.h.8.attn.c_attn', 'transformer.h.8.attn.c_proj', 'transformer.h.8.mlp.c_fc', 'transformer.h.8.mlp.c_proj', 'transformer.h.9.attn.c_attn', 'transformer.h.9.attn.c_proj', 'transformer.h.9.mlp.c_fc', 'transformer.h.9.mlp.c_proj'])\n",
      "dict_keys(['lm_head', 'transformer.h.0.attn.c_attn', 'transformer.h.0.attn.c_proj', 'transformer.h.0.mlp.c_fc', 'transformer.h.0.mlp.c_proj', 'transformer.h.1.attn.c_attn', 'transformer.h.1.attn.c_proj', 'transformer.h.1.mlp.c_fc', 'transformer.h.1.mlp.c_proj', 'transformer.h.10.attn.c_attn', 'transformer.h.10.attn.c_proj', 'transformer.h.10.mlp.c_fc', 'transformer.h.10.mlp.c_proj', 'transformer.h.11.attn.c_attn', 'transformer.h.11.attn.c_proj', 'transformer.h.11.mlp.c_fc', 'transformer.h.11.mlp.c_proj', 'transformer.h.2.attn.c_attn', 'transformer.h.2.attn.c_proj', 'transformer.h.2.mlp.c_fc', 'transformer.h.2.mlp.c_proj', 'transformer.h.3.attn.c_attn', 'transformer.h.3.attn.c_proj', 'transformer.h.3.mlp.c_fc', 'transformer.h.3.mlp.c_proj', 'transformer.h.4.attn.c_attn', 'transformer.h.4.attn.c_proj', 'transformer.h.4.mlp.c_fc', 'transformer.h.4.mlp.c_proj', 'transformer.h.5.attn.c_attn', 'transformer.h.5.attn.c_proj', 'transformer.h.5.mlp.c_fc', 'transformer.h.5.mlp.c_proj', 'transformer.h.6.attn.c_attn', 'transformer.h.6.attn.c_proj', 'transformer.h.6.mlp.c_fc', 'transformer.h.6.mlp.c_proj', 'transformer.h.7.attn.c_attn', 'transformer.h.7.attn.c_proj', 'transformer.h.7.mlp.c_fc', 'transformer.h.7.mlp.c_proj', 'transformer.h.8.attn.c_attn', 'transformer.h.8.attn.c_proj', 'transformer.h.8.mlp.c_fc', 'transformer.h.8.mlp.c_proj', 'transformer.h.9.attn.c_attn', 'transformer.h.9.attn.c_proj', 'transformer.h.9.mlp.c_fc', 'transformer.h.9.mlp.c_proj'])\n",
      "dict_keys(['lm_head', 'transformer.h.0.attn.c_attn', 'transformer.h.0.attn.c_proj', 'transformer.h.0.mlp.c_fc', 'transformer.h.0.mlp.c_proj', 'transformer.h.1.attn.c_attn', 'transformer.h.1.attn.c_proj', 'transformer.h.1.mlp.c_fc', 'transformer.h.1.mlp.c_proj', 'transformer.h.10.attn.c_attn', 'transformer.h.10.attn.c_proj', 'transformer.h.10.mlp.c_fc', 'transformer.h.10.mlp.c_proj', 'transformer.h.11.attn.c_attn', 'transformer.h.11.attn.c_proj', 'transformer.h.11.mlp.c_fc', 'transformer.h.11.mlp.c_proj', 'transformer.h.2.attn.c_attn', 'transformer.h.2.attn.c_proj', 'transformer.h.2.mlp.c_fc', 'transformer.h.2.mlp.c_proj', 'transformer.h.3.attn.c_attn', 'transformer.h.3.attn.c_proj', 'transformer.h.3.mlp.c_fc', 'transformer.h.3.mlp.c_proj', 'transformer.h.4.attn.c_attn', 'transformer.h.4.attn.c_proj', 'transformer.h.4.mlp.c_fc', 'transformer.h.4.mlp.c_proj', 'transformer.h.5.attn.c_attn', 'transformer.h.5.attn.c_proj', 'transformer.h.5.mlp.c_fc', 'transformer.h.5.mlp.c_proj', 'transformer.h.6.attn.c_attn', 'transformer.h.6.attn.c_proj', 'transformer.h.6.mlp.c_fc', 'transformer.h.6.mlp.c_proj', 'transformer.h.7.attn.c_attn', 'transformer.h.7.attn.c_proj', 'transformer.h.7.mlp.c_fc', 'transformer.h.7.mlp.c_proj', 'transformer.h.8.attn.c_attn', 'transformer.h.8.attn.c_proj', 'transformer.h.8.mlp.c_fc', 'transformer.h.8.mlp.c_proj', 'transformer.h.9.attn.c_attn', 'transformer.h.9.attn.c_proj', 'transformer.h.9.mlp.c_fc', 'transformer.h.9.mlp.c_proj'])\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "for i in range(len(filesnames_safetensors)):\n",
    "    path = filesnames_safetensors[i]\n",
    "    tensors = {}\n",
    "    with safe_open(path, framework=\"pt\", device=0) as f:\n",
    "        for k in f.keys():\n",
    "            tensors[k] = f.get_tensor(k)\n",
    "    print(tensors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = filesnames_safetensors[-2]\n",
    "with safe_open(path, framework=\"pt\", device=0) as f:\n",
    "    for k in f.keys():\n",
    "        tensors[k] = f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_2 = torch.load(\"/home/louis/quelle/quelle/approx_unrolling/checkpoints/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_modules = []\n",
    "\n",
    "for i in range(12):\n",
    "    total_modules.append(f\"transformer.h.{i}.attn.c_attn\")\n",
    "    total_modules.append(f\"transformer.h.{i}.attn.c_proj\")\n",
    "\n",
    "for i in range(12):\n",
    "    total_modules.append(f\"transformer.h.{i}.mlp.c_fc\")\n",
    "    total_modules.append(f\"transformer.h.{i}.mlp.c_proj\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_modules' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtotal_modules\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'total_modules' is not defined"
     ]
    }
   ],
   "source": [
    "total_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = model.state_dict()\n",
    "\n",
    "all_mlps = {k: v for k, v in all_weights.items() if \"mlp\" in k}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_mlps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_keys = [m[0] for m in model.named_modules()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_attention = True\n",
    "track_mlp = True\n",
    "total_modules = []\n",
    "for m in module_keys:\n",
    "    if \"dropout\" in m.lower() or \"layernorm\" in m.lower():\n",
    "        continue\n",
    "\n",
    "    if \"attention\" in m.lower() and track_attention:\n",
    "        total_modules.append(m)\n",
    "    if \"mlp\" in m.lower() and track_mlp:\n",
    "        total_modules.append(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt_neox.layers.0.attention',\n",
       " 'gpt_neox.layers.0.attention.query_key_value',\n",
       " 'gpt_neox.layers.0.attention.dense',\n",
       " 'gpt_neox.layers.0.mlp',\n",
       " 'gpt_neox.layers.0.mlp.dense_h_to_4h',\n",
       " 'gpt_neox.layers.0.mlp.dense_4h_to_h',\n",
       " 'gpt_neox.layers.0.mlp.act',\n",
       " 'gpt_neox.layers.1.attention',\n",
       " 'gpt_neox.layers.1.attention.query_key_value',\n",
       " 'gpt_neox.layers.1.attention.dense',\n",
       " 'gpt_neox.layers.1.mlp',\n",
       " 'gpt_neox.layers.1.mlp.dense_h_to_4h',\n",
       " 'gpt_neox.layers.1.mlp.dense_4h_to_h',\n",
       " 'gpt_neox.layers.1.mlp.act',\n",
       " 'gpt_neox.layers.2.attention',\n",
       " 'gpt_neox.layers.2.attention.query_key_value',\n",
       " 'gpt_neox.layers.2.attention.dense',\n",
       " 'gpt_neox.layers.2.mlp',\n",
       " 'gpt_neox.layers.2.mlp.dense_h_to_4h',\n",
       " 'gpt_neox.layers.2.mlp.dense_4h_to_h',\n",
       " 'gpt_neox.layers.2.mlp.act',\n",
       " 'gpt_neox.layers.3.attention',\n",
       " 'gpt_neox.layers.3.attention.query_key_value',\n",
       " 'gpt_neox.layers.3.attention.dense',\n",
       " 'gpt_neox.layers.3.mlp',\n",
       " 'gpt_neox.layers.3.mlp.dense_h_to_4h',\n",
       " 'gpt_neox.layers.3.mlp.dense_4h_to_h',\n",
       " 'gpt_neox.layers.3.mlp.act',\n",
       " 'gpt_neox.layers.4.attention',\n",
       " 'gpt_neox.layers.4.attention.query_key_value',\n",
       " 'gpt_neox.layers.4.attention.dense',\n",
       " 'gpt_neox.layers.4.mlp',\n",
       " 'gpt_neox.layers.4.mlp.dense_h_to_4h',\n",
       " 'gpt_neox.layers.4.mlp.dense_4h_to_h',\n",
       " 'gpt_neox.layers.4.mlp.act',\n",
       " 'gpt_neox.layers.5.attention',\n",
       " 'gpt_neox.layers.5.attention.query_key_value',\n",
       " 'gpt_neox.layers.5.attention.dense',\n",
       " 'gpt_neox.layers.5.mlp',\n",
       " 'gpt_neox.layers.5.mlp.dense_h_to_4h',\n",
       " 'gpt_neox.layers.5.mlp.dense_4h_to_h',\n",
       " 'gpt_neox.layers.5.mlp.act']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention'>\n",
      "gpt_neox.layers.0.attention GPTNeoXAttention(\n",
      "  (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "\n",
      "<class 'str'> <class 'torch.nn.modules.linear.Linear'>\n",
      "gpt_neox.layers.0.attention.query_key_value Linear(in_features=128, out_features=384, bias=True)\n",
      "\n",
      "<class 'str'> <class 'torch.nn.modules.linear.Linear'>\n",
      "gpt_neox.layers.0.attention.dense Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "<class 'str'> <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention'>\n",
      "gpt_neox.layers.1.attention GPTNeoXAttention(\n",
      "  (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "\n",
      "<class 'str'> <class 'torch.nn.modules.linear.Linear'>\n",
      "gpt_neox.layers.1.attention.query_key_value Linear(in_features=128, out_features=384, bias=True)\n",
      "\n",
      "<class 'str'> <class 'torch.nn.modules.linear.Linear'>\n",
      "gpt_neox.layers.1.attention.dense Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "<class 'str'> <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention'>\n",
      "gpt_neox.layers.2.attention GPTNeoXAttention(\n",
      "  (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "\n",
      "<class 'str'> <class 'torch.nn.modules.linear.Linear'>\n",
      "gpt_neox.layers.2.attention.query_key_value Linear(in_features=128, out_features=384, bias=True)\n",
      "\n",
      "<class 'str'> <class 'torch.nn.modules.linear.Linear'>\n",
      "gpt_neox.layers.2.attention.dense Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "<class 'str'> <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention'>\n",
      "gpt_neox.layers.3.attention GPTNeoXAttention(\n",
      "  (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "\n",
      "<class 'str'> <class 'torch.nn.modules.linear.Linear'>\n",
      "gpt_neox.layers.3.attention.query_key_value Linear(in_features=128, out_features=384, bias=True)\n",
      "\n",
      "<class 'str'> <class 'torch.nn.modules.linear.Linear'>\n",
      "gpt_neox.layers.3.attention.dense Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "<class 'str'> <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention'>\n",
      "gpt_neox.layers.4.attention GPTNeoXAttention(\n",
      "  (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "\n",
      "<class 'str'> <class 'torch.nn.modules.linear.Linear'>\n",
      "gpt_neox.layers.4.attention.query_key_value Linear(in_features=128, out_features=384, bias=True)\n",
      "\n",
      "<class 'str'> <class 'torch.nn.modules.linear.Linear'>\n",
      "gpt_neox.layers.4.attention.dense Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "<class 'str'> <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention'>\n",
      "gpt_neox.layers.5.attention GPTNeoXAttention(\n",
      "  (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "\n",
      "<class 'str'> <class 'torch.nn.modules.linear.Linear'>\n",
      "gpt_neox.layers.5.attention.query_key_value Linear(in_features=128, out_features=384, bias=True)\n",
      "\n",
      "<class 'str'> <class 'torch.nn.modules.linear.Linear'>\n",
      "gpt_neox.layers.5.attention.dense Linear(in_features=128, out_features=128, bias=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in model.named_modules():\n",
    "    if \"dropout\" in m[0] or \"layernorm\" in m[0]:\n",
    "        continue\n",
    "    # if \"mlp\" in m[0] and \"dropout\" not in m[0]:\n",
    "    #     print(m[0], m[1])\n",
    "    #     print()\n",
    "    if \"attention\" in m[0]:\n",
    "        print(type(m[0]), type(m[1]))\n",
    "        print(m[0], m[1])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['gpt_neox.embed_in.weight', 'gpt_neox.layers.0.input_layernorm.weight', 'gpt_neox.layers.0.input_layernorm.bias', 'gpt_neox.layers.0.post_attention_layernorm.weight', 'gpt_neox.layers.0.post_attention_layernorm.bias', 'gpt_neox.layers.0.attention.query_key_value.weight', 'gpt_neox.layers.0.attention.query_key_value.bias', 'gpt_neox.layers.0.attention.dense.weight', 'gpt_neox.layers.0.attention.dense.bias', 'gpt_neox.layers.0.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.0.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.0.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.0.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.1.input_layernorm.weight', 'gpt_neox.layers.1.input_layernorm.bias', 'gpt_neox.layers.1.post_attention_layernorm.weight', 'gpt_neox.layers.1.post_attention_layernorm.bias', 'gpt_neox.layers.1.attention.query_key_value.weight', 'gpt_neox.layers.1.attention.query_key_value.bias', 'gpt_neox.layers.1.attention.dense.weight', 'gpt_neox.layers.1.attention.dense.bias', 'gpt_neox.layers.1.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.1.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.1.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.1.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.2.input_layernorm.weight', 'gpt_neox.layers.2.input_layernorm.bias', 'gpt_neox.layers.2.post_attention_layernorm.weight', 'gpt_neox.layers.2.post_attention_layernorm.bias', 'gpt_neox.layers.2.attention.query_key_value.weight', 'gpt_neox.layers.2.attention.query_key_value.bias', 'gpt_neox.layers.2.attention.dense.weight', 'gpt_neox.layers.2.attention.dense.bias', 'gpt_neox.layers.2.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.2.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.2.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.2.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.3.input_layernorm.weight', 'gpt_neox.layers.3.input_layernorm.bias', 'gpt_neox.layers.3.post_attention_layernorm.weight', 'gpt_neox.layers.3.post_attention_layernorm.bias', 'gpt_neox.layers.3.attention.query_key_value.weight', 'gpt_neox.layers.3.attention.query_key_value.bias', 'gpt_neox.layers.3.attention.dense.weight', 'gpt_neox.layers.3.attention.dense.bias', 'gpt_neox.layers.3.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.3.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.3.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.3.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.4.input_layernorm.weight', 'gpt_neox.layers.4.input_layernorm.bias', 'gpt_neox.layers.4.post_attention_layernorm.weight', 'gpt_neox.layers.4.post_attention_layernorm.bias', 'gpt_neox.layers.4.attention.query_key_value.weight', 'gpt_neox.layers.4.attention.query_key_value.bias', 'gpt_neox.layers.4.attention.dense.weight', 'gpt_neox.layers.4.attention.dense.bias', 'gpt_neox.layers.4.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.4.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.4.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.4.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.5.input_layernorm.weight', 'gpt_neox.layers.5.input_layernorm.bias', 'gpt_neox.layers.5.post_attention_layernorm.weight', 'gpt_neox.layers.5.post_attention_layernorm.bias', 'gpt_neox.layers.5.attention.query_key_value.weight', 'gpt_neox.layers.5.attention.query_key_value.bias', 'gpt_neox.layers.5.attention.dense.weight', 'gpt_neox.layers.5.attention.dense.bias', 'gpt_neox.layers.5.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.5.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.5.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.5.mlp.dense_4h_to_h.bias', 'gpt_neox.final_layer_norm.weight', 'gpt_neox.final_layer_norm.bias', 'embed_out.weight'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias', 'lm_head.weight'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from examples.wikitext.pipeline import construct_gpt2\n",
    "\n",
    "model_state_dict = construct_gpt2().state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = model_state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias', 'lm_head.weight'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_modules = []\n",
    "total_modules += [k for k in model_state_dict.keys() if \"attention\" in k]\n",
    "total_modules += [k for k in model_state_dict.keys() if \"mlp\" in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_modules at 0x7149cc173450>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 128)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "          (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  )\n",
      "  (embed_out): Linear(in_features=128, out_features=50304, bias=False)\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox', GPTNeoXModel(\n",
      "  (embed_in): Embedding(50304, 128)\n",
      "  (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-5): 6 x GPTNeoXLayer(\n",
      "      (input_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (attention): GPTNeoXAttention(\n",
      "        (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (mlp): GPTNeoXMLP(\n",
      "        (dense_h_to_4h): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dense_4h_to_h): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (act): GELUActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.embed_in', Embedding(50304, 128))\n",
      "----------------------------------------\n",
      "('gpt_neox.emb_dropout', Dropout(p=0.0, inplace=False))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers', ModuleList(\n",
      "  (0-5): 6 x GPTNeoXLayer(\n",
      "    (input_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (post_attention_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (attention): GPTNeoXAttention(\n",
      "      (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (mlp): GPTNeoXMLP(\n",
      "      (dense_h_to_4h): Linear(in_features=128, out_features=512, bias=True)\n",
      "      (dense_4h_to_h): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (act): GELUActivation()\n",
      "    )\n",
      "  )\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.0', GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.0.input_layernorm', LayerNorm((128,), eps=1e-05, elementwise_affine=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.0.post_attention_layernorm', LayerNorm((128,), eps=1e-05, elementwise_affine=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.0.post_attention_dropout', Dropout(p=0.0, inplace=False))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.0.post_mlp_dropout', Dropout(p=0.0, inplace=False))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.0.attention', GPTNeoXAttention(\n",
      "  (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.0.attention.query_key_value', Linear(in_features=128, out_features=384, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.0.attention.dense', Linear(in_features=128, out_features=128, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.0.mlp', GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (act): GELUActivation()\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.0.mlp.dense_h_to_4h', Linear(in_features=128, out_features=512, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.0.mlp.dense_4h_to_h', Linear(in_features=512, out_features=128, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.0.mlp.act', GELUActivation())\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.1', GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.1.input_layernorm', LayerNorm((128,), eps=1e-05, elementwise_affine=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.1.post_attention_layernorm', LayerNorm((128,), eps=1e-05, elementwise_affine=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.1.post_attention_dropout', Dropout(p=0.0, inplace=False))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.1.post_mlp_dropout', Dropout(p=0.0, inplace=False))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.1.attention', GPTNeoXAttention(\n",
      "  (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.1.attention.query_key_value', Linear(in_features=128, out_features=384, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.1.attention.dense', Linear(in_features=128, out_features=128, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.1.mlp', GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (act): GELUActivation()\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.1.mlp.dense_h_to_4h', Linear(in_features=128, out_features=512, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.1.mlp.dense_4h_to_h', Linear(in_features=512, out_features=128, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.1.mlp.act', GELUActivation())\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.2', GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.2.input_layernorm', LayerNorm((128,), eps=1e-05, elementwise_affine=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.2.post_attention_layernorm', LayerNorm((128,), eps=1e-05, elementwise_affine=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.2.post_attention_dropout', Dropout(p=0.0, inplace=False))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.2.post_mlp_dropout', Dropout(p=0.0, inplace=False))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.2.attention', GPTNeoXAttention(\n",
      "  (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.2.attention.query_key_value', Linear(in_features=128, out_features=384, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.2.attention.dense', Linear(in_features=128, out_features=128, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.2.mlp', GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (act): GELUActivation()\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.2.mlp.dense_h_to_4h', Linear(in_features=128, out_features=512, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.2.mlp.dense_4h_to_h', Linear(in_features=512, out_features=128, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.2.mlp.act', GELUActivation())\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.3', GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.3.input_layernorm', LayerNorm((128,), eps=1e-05, elementwise_affine=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.3.post_attention_layernorm', LayerNorm((128,), eps=1e-05, elementwise_affine=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.3.post_attention_dropout', Dropout(p=0.0, inplace=False))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.3.post_mlp_dropout', Dropout(p=0.0, inplace=False))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.3.attention', GPTNeoXAttention(\n",
      "  (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.3.attention.query_key_value', Linear(in_features=128, out_features=384, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.3.attention.dense', Linear(in_features=128, out_features=128, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.3.mlp', GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (act): GELUActivation()\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.3.mlp.dense_h_to_4h', Linear(in_features=128, out_features=512, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.3.mlp.dense_4h_to_h', Linear(in_features=512, out_features=128, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.3.mlp.act', GELUActivation())\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.4', GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.4.input_layernorm', LayerNorm((128,), eps=1e-05, elementwise_affine=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.4.post_attention_layernorm', LayerNorm((128,), eps=1e-05, elementwise_affine=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.4.post_attention_dropout', Dropout(p=0.0, inplace=False))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.4.post_mlp_dropout', Dropout(p=0.0, inplace=False))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.4.attention', GPTNeoXAttention(\n",
      "  (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.4.attention.query_key_value', Linear(in_features=128, out_features=384, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.4.attention.dense', Linear(in_features=128, out_features=128, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.4.mlp', GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (act): GELUActivation()\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.4.mlp.dense_h_to_4h', Linear(in_features=128, out_features=512, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.4.mlp.dense_4h_to_h', Linear(in_features=512, out_features=128, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.4.mlp.act', GELUActivation())\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.5', GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.5.input_layernorm', LayerNorm((128,), eps=1e-05, elementwise_affine=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.5.post_attention_layernorm', LayerNorm((128,), eps=1e-05, elementwise_affine=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.5.post_attention_dropout', Dropout(p=0.0, inplace=False))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.5.post_mlp_dropout', Dropout(p=0.0, inplace=False))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.5.attention', GPTNeoXAttention(\n",
      "  (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.5.attention.query_key_value', Linear(in_features=128, out_features=384, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.5.attention.dense', Linear(in_features=128, out_features=128, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.5.mlp', GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (act): GELUActivation()\n",
      "))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.5.mlp.dense_h_to_4h', Linear(in_features=128, out_features=512, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.5.mlp.dense_4h_to_h', Linear(in_features=512, out_features=128, bias=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.layers.5.mlp.act', GELUActivation())\n",
      "----------------------------------------\n",
      "('gpt_neox.final_layer_norm', LayerNorm((128,), eps=1e-05, elementwise_affine=True))\n",
      "----------------------------------------\n",
      "('gpt_neox.rotary_emb', GPTNeoXRotaryEmbedding())\n",
      "----------------------------------------\n",
      "('embed_out', Linear(in_features=128, out_features=50304, bias=False))\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for m in model.named_modules():\n",
    "    print(m)\n",
    "    print(\"--\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_modules = []\n",
    "\n",
    "for i in range(12):\n",
    "    total_modules.append(f\"transformer.h.{i}.attn.c_attn\")\n",
    "    total_modules.append(f\"transformer.h.{i}.attn.c_proj\")\n",
    "\n",
    "for i in range(12):\n",
    "    total_modules.append(f\"transformer.h.{i}.mlp.c_fc\")\n",
    "    total_modules.append(f\"transformer.h.{i}.mlp.c_proj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quelle.approx_unrolling.language_task import LanguageModelingTask\n",
    "from kronfluence.module.tracked_module import TrackedModule\n",
    "\n",
    "model = model\n",
    "task = LanguageModelingTask()\n",
    "factor_args = (None,)\n",
    "score_args = (None,)\n",
    "\n",
    "\n",
    "tracked_module_names = (\n",
    "    task.get_influence_tracked_modules() if task is not None else None\n",
    ")\n",
    "tracked_module_exists_dict = None\n",
    "if tracked_module_names is not None:\n",
    "    tracked_module_exists_dict = {name: False for name in tracked_module_names}\n",
    "per_sample_gradient_process_fnc = None\n",
    "if task is not None and task.enable_post_process_per_sample_gradient:\n",
    "    per_sample_gradient_process_fnc = task.post_process_per_sample_gradient\n",
    "\n",
    "named_modules = model.named_modules()\n",
    "for module_name, module in named_modules:\n",
    "    if len(list(module.children())) > 0:\n",
    "        continue\n",
    "\n",
    "    # Filters modules based on the task's `get_influence_tracked_modules` if specified.\n",
    "    if tracked_module_names is not None and module_name not in tracked_module_names:\n",
    "        continue\n",
    "\n",
    "    # Wraps the module if it is currently supported (e.g., nn.Linear & nn.Conv2d).\n",
    "    if isinstance(module, tuple(TrackedModule.SUPPORTED_MODULES)):\n",
    "        tracked_module = TrackedModule.SUPPORTED_MODULES[type(module)](\n",
    "            name=module_name,\n",
    "            original_module=module,\n",
    "            per_sample_gradient_process_fnc=per_sample_gradient_process_fnc,\n",
    "            factor_args=factor_args,\n",
    "            score_args=score_args,\n",
    "        )\n",
    "        parent, target_name = _get_submodules(model=model, key=module_name)\n",
    "        setattr(parent, target_name, tracked_module)\n",
    "\n",
    "        if tracked_module_exists_dict is not None:\n",
    "            tracked_module_exists_dict[module_name] = True\n",
    "\n",
    "if tracked_module_exists_dict is not None and not all(\n",
    "    list(tracked_module_exists_dict.values())\n",
    "):\n",
    "    error_msg = f\"Some provided tracked modules were not found. The current mapping: `{tracked_module_exists_dict}`.\"\n",
    "    raise IllegalTaskConfigurationError(error_msg)\n",
    "\n",
    "if not any(isinstance(module, TrackedModule) for module in model.modules()):\n",
    "    supported_modules = \", \".join(\n",
    "        module.__name__ for module in TrackedModule.SUPPORTED_MODULES\n",
    "    )\n",
    "    raise IllegalTaskConfigurationError(\n",
    "        f\"No supported modules found. Kronfluence supports: {supported_modules}. \"\n",
    "        \"Consider rewriting your model or subclassing `TrackedModule` for custom layers.\\n\"\n",
    "        f\"Current Model:\\n{model}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    R = torch.randn(int(1e8), 100, dtype=torch.float32, device=\"cuda\")\n",
    "    U, S, Vh = torch.svd_lowrank(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdist\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mdist\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit_process_group\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnccl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m dist.get_rank()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/quelle/.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81\u001b[39m, in \u001b[36m_exception_logger.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m     83\u001b[39m         msg_dict = _get_msg_dict(func.\u001b[34m__name__\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/quelle/.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95\u001b[39m, in \u001b[36m_time_logger.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _WaitCounter(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpytorch.wait_counter.c10d.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m).guard():\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         func_return = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func_return\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/quelle/.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1710\u001b[39m, in \u001b[36minit_process_group\u001b[39m\u001b[34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options, device_id)\u001b[39m\n\u001b[32m   1706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1707\u001b[39m     rendezvous_iterator = rendezvous(\n\u001b[32m   1708\u001b[39m         not_none(init_method), rank, world_size, timeout=timeout\n\u001b[32m   1709\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1710\u001b[39m     store, rank, world_size = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrendezvous_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1711\u001b[39m     store.set_timeout(timeout)\n\u001b[32m   1713\u001b[39m     \u001b[38;5;66;03m# Use a PrefixStore to avoid accidental overrides of keys used by\u001b[39;00m\n\u001b[32m   1714\u001b[39m     \u001b[38;5;66;03m# different systems (e.g. RPC) in case the store is multi-tenant.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/quelle/.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:267\u001b[39m, in \u001b[36m_env_rendezvous_handler\u001b[39m\u001b[34m(url, timeout, **kwargs)\u001b[39m\n\u001b[32m    265\u001b[39m     rank = \u001b[38;5;28mint\u001b[39m(query_dict[\u001b[33m\"\u001b[39m\u001b[33mrank\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     rank = \u001b[38;5;28mint\u001b[39m(\u001b[43m_get_env_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRANK\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mworld_size\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m query_dict:\n\u001b[32m    270\u001b[39m     world_size = \u001b[38;5;28mint\u001b[39m(query_dict[\u001b[33m\"\u001b[39m\u001b[33mworld_size\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/quelle/.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:252\u001b[39m, in \u001b[36m_env_rendezvous_handler.<locals>._get_env_or_raise\u001b[39m\u001b[34m(env_var)\u001b[39m\n\u001b[32m    250\u001b[39m env_val = os.environ.get(env_var, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_val:\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _env_error(env_var)\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_val\n",
      "\u001b[31mValueError\u001b[39m: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdist\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mdist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/quelle/.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:2291\u001b[39m, in \u001b[36mget_rank\u001b[39m\u001b[34m(group)\u001b[39m\n\u001b[32m   2288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _rank_not_in_group(group):\n\u001b[32m   2289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2291\u001b[39m default_pg = \u001b[43m_get_default_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m GroupMember.WORLD:\n\u001b[32m   2293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m default_pg.rank()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/quelle/.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1298\u001b[39m, in \u001b[36m_get_default_group\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1296\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the default process group created by init_process_group.\"\"\"\u001b[39;00m\n\u001b[32m   1297\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_initialized():\n\u001b[32m-> \u001b[39m\u001b[32m1298\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1299\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDefault process group has not been initialized, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1300\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mplease make sure to call init_process_group.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1301\u001b[39m     )\n\u001b[32m   1302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m   1303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m not_none(GroupMember.WORLD)\n",
      "\u001b[31mValueError\u001b[39m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
